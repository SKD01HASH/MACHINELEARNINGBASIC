{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Day-9-Session-2.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"sst0tyMjHNtE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628099238784,"user_tz":-330,"elapsed":2915,"user":{"displayName":"Atul Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Sj6HbEgKSyeFNDJ-LeKBhjDnTp4odh40aPONsg=s64","userId":"02777987882236980528"}},"outputId":"c34403fb-d1ed-465a-c56a-3a57e2d16e39"},"source":["import nltk as nl\n","nl.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"de3HxSU_HNtR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628075231281,"user_tz":-330,"elapsed":431,"user":{"displayName":"Atul Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Sj6HbEgKSyeFNDJ-LeKBhjDnTp4odh40aPONsg=s64","userId":"02777987882236980528"}},"outputId":"d7ac5c89-9f62-4023-b831-deed5d38e6f7"},"source":["#sentance tokenization\n","import nltk as nl\n","#import random\n","example_text = 'a real movie , about real people. Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n","print(nl.tokenize.sent_tokenize(example_text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['a real movie , about real people.', \"Movie gives us a rare glimpse into a culture most of us don't know .\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lGRN18RlHNtZ","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"3bf3043f-c591-4a22-e1e5-78fbaeec6582"},"source":["#word tokenization\n","example = 'Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n","\n","tokenized_words = nl.tokenize.word_tokenize(example)\n","print(tokenized_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Movie', 'gives', 'us', 'a', 'rare', 'glimpse', 'into', 'a', 'culture', 'most', 'of', 'us', 'do', \"n't\", 'know', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JWQJO7xGHNtf","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"86a32e6d-bde7-499c-8484-ca17ab526ac2"},"source":["#pos tagging\n","nl.download('averaged_perceptron_tagger')\n","tagged_words = nl.tag.pos_tag(tokenized_words)\n","print(tagged_words)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[('Movie', 'NNP'), ('gives', 'VBZ'), ('us', 'PRP'), ('a', 'DT'), ('rare', 'JJ'), ('glimpse', 'NN'), ('into', 'IN'), ('a', 'DT'), ('culture', 'NN'), ('most', 'JJS'), ('of', 'IN'), ('us', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LDM_9G6xHNtj","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"4df0ed09-d3b8-4f8a-a07f-e5076ee3fef5"},"source":["# removal of stop words\n","nl.download('stopwords')\n","\n","stop_words = set(nl.corpus.stopwords.words('english'))\n","\n","word_tokens = nl.tokenize.word_tokenize(example)\n","\n","filtered_words = []\n","\n","for w in word_tokens:\n","    if w not in stop_words:\n","        filtered_words.append(w)\n","print('stop words provided by NLTK package----\\n')\n","print(stop_words)\n","print('\\n filtered data after stop word removal \\n')\n","print(filtered_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","stop words provided by NLTK package----\n","\n","{'y', 'yourself', \"hadn't\", 'didn', 'which', 'they', 'a', 'against', 'do', 'theirs', 'aren', 'all', 'myself', 'weren', 'through', 'because', 'wouldn', 'i', 'isn', \"won't\", 'same', 'during', 'such', 'are', \"should've\", 'too', 'themselves', 'did', \"mustn't\", 'the', 'at', 'has', \"don't\", 'before', 'can', 'no', 'their', 'am', 'couldn', 'our', 'ain', 'each', 'where', \"needn't\", 'you', 'between', \"weren't\", 'if', \"you've\", 'more', 'had', 'his', 'o', 'few', 'd', \"isn't\", 'other', 'an', 'only', 'most', 'again', 'its', 'mightn', 'once', 'who', 'or', \"wasn't\", 'your', 'mustn', 'some', 'over', 'not', \"didn't\", 'when', 'doesn', 'been', 'were', 'her', 've', 'does', \"you'd\", 'into', \"couldn't\", 'yourselves', 'than', 'll', 'why', 'is', 'whom', 't', 'doing', 'shouldn', \"aren't\", 'down', \"doesn't\", \"you're\", 'off', 'wasn', 'what', 'my', 'those', 'she', 'both', 'very', 'as', 'it', 'on', 'needn', 'after', 'we', 'so', \"mightn't\", 'of', \"shouldn't\", 'don', 'further', 'hers', 'herself', 'have', \"haven't\", 'and', 'out', 'ma', 'itself', 'shan', \"it's\", 'hadn', \"she's\", 'will', 'this', 'yours', 'then', 'ours', 'any', 'up', 'him', 'by', 'in', \"that'll\", 'with', 'that', 'was', 'while', 're', 'for', \"shan't\", 'under', 'to', 'being', 'hasn', 'these', 'himself', 'just', 'about', 'own', 'ourselves', 'how', 'm', 'me', 'but', 'now', 'nor', \"you'll\", 's', 'there', 'he', 'won', 'above', 'below', 'be', 'should', \"wouldn't\", 'until', 'here', 'haven', \"hasn't\", 'them', 'having', 'from'}\n","\n"," filtered data after stop word removal \n","\n","['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cC2t3IvLHNtq","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"68799e5c-f983-4b8e-9204-d654cfa8a477"},"source":["#punctuation mark removal\n","p = [',','?','.']\n","word_tokens = filtered_words\n","\n","\n","filtered2 = []\n","\n","for w in word_tokens:\n","    if w not in p:\n","        filtered2.append(w)\n","\n","print(word_tokens)\n","print(filtered2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n","['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ffwp_QfBHNtt","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"72228427-d991-40bd-c44f-48128784bece"},"source":["# stemming\n","\n","ps = nl.stem.PorterStemmer()\n","set1 =['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n","\n","for w in set1:\n","    print(ps.stem(w))\n","    \n","set2 =[ \"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n","set2 =[ \"banker\",\"banking\",\"bank\"]\n","\n","print('-------------------')\n","for w in set2:\n","    print(ps.stem(w))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["movi\n","give\n","us\n","rare\n","glimps\n","cultur\n","us\n","n't\n","know\n","-------------------\n","banker\n","bank\n","bank\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7WyN-XpXHNtv","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"9726380a-2dc2-474e-9c44-cf1d8dde277d"},"source":["#data set loading\n","positive = open('rt-polarity-pos.txt', encoding='latin1')\n","negative = open('rt-polarity-neg.txt', encoding='latin1')\n","\n","i=0\n","while i<5 :\n","    print(negative.readline())\n","    i+=1\n","\n","#print(positive.readlines())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["simplistic , silly and tedious . \n","\n","it's so laddish and juvenile , only teenage boys could possibly find it funny . \n","\n","exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n","\n","[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n","\n","a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PfP_gAMPHNty","colab":{"base_uri":"https://localhost:8080/","height":557},"outputId":"716227e1-ef6a-4268-b72d-b4be610e76f3"},"source":["#preprocessing\n","def remove_stop_words(w_token):\n","    stop_words = set(nl.corpus.stopwords.words('english'))\n","    filtered_words = []\n","    ps = nl.stem.PorterStemmer()\n","    for tmp_word in w_token:\n","        if tmp_word not in stop_words:\n","            filtered_words.append(tmp_word)\n","    return filtered_words\n","    \n","def process_sentence(s):\n","    w_token = nl.tokenize.word_tokenize(s) \n","    punctuations = [',','?','.',']','[','}','{','(',')','!','?',':',';','\"','\\'']\n","    t2 = []\n","    for w in w_token:\n","        if w not in punctuations:\n","            t2.append(w)\n","    t3 = remove_stop_words(t2)\n","    return {word: 1 for word in t3}\n","    \n","\n","positive_data_array = []\n","\n","# positive review prcocessing\n","for p_review in positive:\n","    positive_data_array.append([process_sentence(p_review),'positive'])\n","        \n","\n","\n","negative_data_array = []\n","i= 0\n","\n","#negative review processing\n","for n_review in negative:\n","        processed = [process_sentence(n_review),'negative']   \n","        negative_data_array.append(processed)\n","        \n","        if(i<5):\n","            print('review before processing->')            # demo purpose code to see the affect\n","            print(n_review)\n","            print('review after processing->')\n","            print(processed)\n","            print('-------------------------')\n","            i+=1\n","        \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["review before processing->\n","the story is also as unoriginal as they come , already having been recycled more times than i'd care to count . \n","\n","review after processing->\n","[{'story': 1, 'also': 1, 'unoriginal': 1, 'come': 1, 'already': 1, 'recycled': 1, 'times': 1, \"'d\": 1, 'care': 1, 'count': 1}, 'negative']\n","-------------------------\n","review before processing->\n","about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time . \n","\n","review after processing->\n","[{'thing': 1, 'give': 1, 'movie': 1, 'points': 1, 'bravado': 1, '--': 1, 'take': 1, 'entirely': 1, 'stale': 1, 'concept': 1, 'push': 1, 'audience': 1, \"'s\": 1, 'meat': 1, 'grinder': 1, 'one': 1, 'time': 1}, 'negative']\n","-------------------------\n","review before processing->\n","not so much farcical as sour . \n","\n","review after processing->\n","[{'much': 1, 'farcical': 1, 'sour': 1}, 'negative']\n","-------------------------\n","review before processing->\n","unfortunately the story and the actors are served with a hack script . \n","\n","review after processing->\n","[{'unfortunately': 1, 'story': 1, 'actors': 1, 'served': 1, 'hack': 1, 'script': 1}, 'negative']\n","-------------------------\n","review before processing->\n","all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject . \n","\n","review after processing->\n","[{'disquieting': 1, 'relatively': 1, 'gore-free': 1, 'allusions': 1, 'serial': 1, 'murders': 1, 'falls': 1, 'attempts': 1, 'humanize': 1, 'subject': 1}, 'negative']\n","-------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QyvuoT5PHNt0"},"source":["#partition into training and test set\n","\n","\n","# shuffling\n","import random\n","random.shuffle(positive_data_array)\n","random.shuffle(negative_data_array)\n","\n","#partitioning\n","training_set = positive_data_array[:3000]+negative_data_array[:3000]\n","test_set =positive_data_array[3000:]+negative_data_array[3000:]\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqYyT-xEHNt2","colab":{"base_uri":"https://localhost:8080/","height":399},"outputId":"92569f81-0a22-442d-b916-9136e4614ad5"},"source":["#build classifier and test\n","classifier = nl.NaiveBayesClassifier.train(training_set)\n","\n","\n","print('Accuracy: ',nl.classify.util.accuracy(classifier,test_set))\n","\n","\n","print('\\n')\n","\n","TP = 0 \n","TN = 0\n","FP = 0\n","FN = 0\n","for itr in range(len(test_set)):\n","\n","    \n","    out=classifier.classify(test_set[itr][0])\n"," \n","    if out=='positive' and test_set[itr][1]=='positive':\n","        TP = TP + 1\n","    if out=='negative' and test_set[itr][1]=='negative':\n","        TN = TN + 1\n","    if out=='positive' and test_set[itr][1]=='negative':\n","        FP = FP + 1\n","    if out=='negative' and test_set[itr][1]=='positive':\n","        FN = FN + 1\n","\n","print('True Positive:',TP,'\\t False Positive:',FP)\n","print('False Negative:',FN,'\\t True Negative:',TN)\n","\n","print('\\n')\n","print('Confusion Matrix:\\n')\n","print('_________________________________________________')\n","print('\\t\\t\\tActual Output\\n')\n","print('_________________________________________________')\n","print('\\t\\t',TP,'\\t\\t',FP)\n","print('Test Output -------------------------------------')\n","print('\\t\\t',FN,'\\t\\t',TN)\n","print('_________________________________________________')\n","print('\\n')\n","print('Precision:[TP/(TP+FP)]\\t',TP/(TP+FP))\n","print('Recall:[TP/(TP+FN)]\\t',TP/(TP+FN))\n","print('Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\\t',(TP+TN)/(TP+TN+FP+FN))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.751556796220743\n","\n","\n","True Positive: 1815 \t False Positive: 641\n","False Negative: 516 \t True Negative: 1685\n","\n","\n","Confusion Matrix:\n","\n","_________________________________________________\n","\t\t\tActual Output\n","\n","_________________________________________________\n","\t\t 1815 \t\t 641\n","Test Output -------------------------------------\n","\t\t 516 \t\t 1685\n","_________________________________________________\n","\n","\n","Precision:[TP/(TP+FP)]\t 0.7390065146579805\n","Recall:[TP/(TP+FN)]\t 0.7786357786357786\n","Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\t 0.751556796220743\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"usC3qNynHNt4","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a71390cf-0fcf-4c62-e633-895376e278a6"},"source":["# classifiy new test instance\n","\n","print(classifier.classify(process_sentence('very bad movie. I have wasted my money.')))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["negative\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vFBDVAodHNt6","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7b19537f-1448-455c-e8e9-83b5e5098631"},"source":["print(classifier.classify(process_sentence('One of the best movie, I have ever seen.')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TF0x49PrHNt8","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"1354f584-fcea-42ff-c67d-5dc52f9b883e"},"source":["# Review taken from the PPT & pre-processed in above code cells\n","\n","print(classifier.classify(process_sentence('a real movie , about real people , that gives us a rare glimpse into a culture most of us don\\'t know .')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["positive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XvCNQwGVHNt-"},"source":[""],"execution_count":null,"outputs":[]}]}